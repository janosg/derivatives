{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from utilities import chol_params_to_lower_triangular_matrix\n",
    "from utilities import cov_matrix_to_sdcorr_params\n",
    "from utilities import number_of_triangular_elements_to_dimension\n",
    "from utilities import dimension_to_number_of_triangular_elements\n",
    "\n",
    "from utilities import commutation_matrix\n",
    "from utilities import elimination_matrix\n",
    "from utilities import duplication_matrix\n",
    "from utilities import transformation_matrix\n",
    "\n",
    "from jax import jacfwd\n",
    "from kernel_transformations_jax import covariance_from_internal as covariance_from_internal_jax\n",
    "from kernel_transformations_jax import sdcorr_from_internal as sdcorr_from_internal_jax\n",
    "from kernel_transformations_jax import probability_from_internal as probability_from_internal_jax\n",
    "\n",
    "from numpy.testing import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{\\text{vec}}\n",
    "\\left (\n",
    "\\begin{matrix}\n",
    "(0,0)  &        &        &        \\\\\n",
    "(1, 0) & (1,1)  &        &        \\\\\n",
    "(2, 0) & (2, 1) & (2, 2) &        \\\\\n",
    "(3, 0) & (3, 1) & (3, 2) & (3, 3) \\\\\n",
    "\\end{matrix}\n",
    "\\right ) =:\n",
    "\\tilde{\\text{vec}}(L) = \n",
    "\\big ( (0,0), (1,0), (1,1), (2,0), (2, 1), (2, 2), (3, 0), (3, 1), (3, 2), (3, 3) \\big )^\\top := v\n",
    "$$\n",
    "\n",
    "The following two functions allow us to move between these two representation of a (lower-triangular) matrix in a bijective fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VALUE = 500\n",
    "\n",
    "SEQUENCE_I = list(itertools.chain.from_iterable(itertools.repeat(i-1, i) for i in range(1, MAX_VALUE)))\n",
    "SEQUENCE_J = list(itertools.chain.from_iterable(range(i-1) for i in range(1, MAX_VALUE)))\n",
    "\n",
    "def _vectorized_index_to_matrix_index(index):\n",
    "    return SEQUENCE_I[index], SEQUENCE_J[index]\n",
    "\n",
    "def _matrix_index_to_vectorized_index(i, j):\n",
    "    return int(i * (i + 1) / 2) +  j\n",
    "\n",
    "for k in range(100):\n",
    "    assert _matrix_index_to_vectorized_index(*_vectorized_index_to_matrix_index(k)) == k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq(n_list, max_n=10):\n",
    "    n = len(n_list)\n",
    "    if n < max_n:\n",
    "        n_list.append(n_list[-1] + n + 1)\n",
    "        n_list = seq(n_list, max_n)\n",
    "        \n",
    "    return n_list\n",
    "\n",
    "\n",
    "TO_DROP_SEQUENCE = seq([0], max_n=500)\n",
    "SEQUENCE_SDCORR = [e for e in range(125250) if e not in TO_DROP_SEQUENCE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ``covariance_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph we want to differentiate looks as follow\n",
    "\n",
    "$$\\tilde{\\text{vec}}(L) \\to L \\to L L^\\top =: \\Sigma \\to \\tilde{\\text{vec}}(\\Sigma) \\,,$$\n",
    "where $L$ is the Cholesky factor of the covariance matrix $\\Sigma$.\n",
    "Let us define a function $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto f(x)$, which takes an internal vector $x$ (of correct dimension) and transforms it to the covariance matrix as depicted above.\n",
    "We want to find the Jacobian of $f$, i.e.\n",
    "$$\n",
    "J(f) = \\left ( \\frac{\\partial \\, f_i}{\\partial \\, x_j} \\right )_{i, j = 1, \\dots, m}\n",
    "$$\n",
    "\n",
    "We tackle this problem by finding an explicit expression for ${\\partial f_i}/{\\partial x_j}$ and then looping over $i,j = 1,\\dots, m$.\n",
    "\n",
    "Henceforth let $i, j$ be given and let $\\Sigma = (\\sigma_{i, j})$.\n",
    "Note that for each $j$ we can find a unique index tuple $(a, b) = (a(j), b(j))$ such that $\\tilde{\\text{vec}}(L)_j = L_{a, b}$ and equivalently we find $(n, m) = (n(i), m(i))$ such that $\\tilde{\\text{vec}}(\\Sigma)_i = \\sigma_{n, m}$. Also note that for these indices it always holds that $a \\geq b$ and $n \\geq m$.\n",
    "\n",
    "Now note again that with $L = \\left [ \\begin{matrix} \\ell_1 \\\\ \\vdots \\\\ \\ell_m \\end{matrix} \\right ]$, we have $\\sigma_{k, l} = \\ell_k^ \\, \\bullet \\ell_l$.\n",
    "Hence we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, f_i}{\\partial \\, x_j} = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\ell_n \\bullet \\, \\ell_m \\right ) = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\sum_{k=1}^{m} L_{n, k} L_{m, k} \\right ) = \n",
    "\\mathbb{1}(b \\leq m) \\left [ \\mathbb{1}(a = n) L_{m, b} + \\mathbb{1}(a = m) L_{n, b} \\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_covariance_from_internal(internal_values):\n",
    "    chol = chol_params_to_lower_triangular_matrix(internal_values)\n",
    "    \n",
    "    dim = len(chol)\n",
    "    \n",
    "    K = commutation_matrix(dim)\n",
    "    \n",
    "    d0_a = np.eye(dim ** 2) + K\n",
    "    d0_b = np.kron(chol, np.eye(dim))\n",
    "    d0 = d0_a @ d0_b\n",
    "    \n",
    "    L = elimination_matrix(dim)\n",
    "    D = duplication_matrix(dim)\n",
    "    \n",
    "    d1 = L @ d0 @ L.T\n",
    "    \n",
    "    return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(covariance_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_internal(dim, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    chol = np.tril(np.random.randn(dim, dim))\n",
    "    internal = chol[np.tril_indices(len(chol))]\n",
    "    return internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(10, 50):\n",
    "    internal = get_random_internal(dim)\n",
    "\n",
    "    jax_deriv = J(internal)\n",
    "\n",
    "    my_deriv = derivative_covariance_from_internal(internal)\n",
    "\n",
    "    assert_array_almost_equal(jax_deriv, my_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.7 ms ± 332 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_covariance_from_internal(internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of ``sdcorr_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph we want to differentiate looks as follow\n",
    "\n",
    "$$\\tilde{\\text{vec}}(L) \\to L \\to L L^\\top =: \\Sigma \\to \\mathcal{P} \\to \\bar{\\text{vec}}(\\mathcal{P}) \\,,$$\n",
    "where $L$ is the Cholesky factor of the covariance matrix $\\Sigma$ and $\\mathcal{P}$ denotes the *modified* correlation matrix. With modified correlation matrix we mean a correlation matrix that has the standard deviations written on the diagonal instead of ones. Moreover, the new operate $\\bar{\\text{vec}}$ maps the diagonal elements of a matrix to the first entries of the resulting vector and proceeds to fill the remaining entries using the operator $\\tilde{\\text{vec}}$. That is, if we let $M$ be some lower-triangular, $m \\times m$ matrix and define $M'$ as the $(m-1) \\times (m-1)$ lower-triangular matrix containing the lower-triangular elements of $M$ except for the diagonal elements, then\n",
    "\n",
    "$$ \\bar{\\text{vec}}(M) = \\left ( \\text{diag}(M), \\tilde{\\text{vec}}(M') \\right )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(9).reshape(3, 3).ravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sdcorr_from_internal(internal_values):\n",
    "    \n",
    "    X = chol_params_to_lower_triangular_matrix(internal_values)\n",
    "    dim = len(X)\n",
    "    \n",
    "    I = np.eye(dim)\n",
    "    S = X @ X.T\n",
    "    A = np.multiply(I, X)\n",
    "    #A = np.diag(np.diag(X))\n",
    "    V = np.linalg.inv(A)\n",
    "    P = V @ S @ V + A - I\n",
    "    \n",
    "    K = commutation_matrix(dim)\n",
    "    Y = np.diag(I.ravel('F'))\n",
    "    \n",
    "    N = np.kron(I, X) @ K + np.kron(X, I)\n",
    "    \n",
    "    VS = V @ S\n",
    "    B = np.kron(V, V)\n",
    "    H = np.kron(VS, I)\n",
    "    J = np.kron(I, VS)\n",
    "    \n",
    "    dpdx = Y + B @ N - (H + J) @ B @ Y\n",
    "    \n",
    "    T = transformation_matrix(dim)\n",
    "    D = duplication_matrix(dim)\n",
    "    \n",
    "    return T @ dpdx @ D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplication_matrix(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplication_matrix(3) @ elimination_matrix(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(sdcorr_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = np.array([1., 2, 4, 6, 1, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "             [ 0.  ,  0.45,  0.89,  0.  ,  0.  ,  0.  ],\n",
       "             [ 0.  ,  0.  ,  0.  ,  0.98,  0.16,  0.08],\n",
       "             [ 0.  ,  0.18, -0.09,  0.  ,  0.  ,  0.  ],\n",
       "             [ 0.  ,  0.  ,  0.  ,  0.01, -0.03, -0.01],\n",
       "             [ 0.  ,  0.16, -0.08, -0.02,  0.13, -0.01]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J(internal).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00e+00,  0.00e+00,  0.00e+00,  0.00e+00,  0.00e+00,  0.00e+00],\n",
       "       [ 0.00e+00,  2.50e-01,  8.80e-01,  0.00e+00,  0.00e+00,  0.00e+00],\n",
       "       [ 0.00e+00,  0.00e+00,  0.00e+00,  4.80e+01,  8.00e+00, -5.91e+02],\n",
       "       [ 0.00e+00,  2.50e-01, -1.20e-01,  0.00e+00,  0.00e+00,  0.00e+00],\n",
       "       [ 0.00e+00,  0.00e+00,  0.00e+00,  2.00e+00,  0.00e+00, -2.40e+01],\n",
       "       [ 0.00e+00,  3.00e+00, -1.50e+00,  1.00e+00,  2.00e+00, -1.60e+01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_sdcorr_from_internal(internal).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(10, 50):\n",
    "    internal = get_random_internal(dim)\n",
    "    jax_deriv = J(internal)\n",
    "    my_deriv = derivative_sdcorr_from_internal(internal)\n",
    "    assert_array_almost_equal(jax_deriv, my_deriv)\n",
    "    bad.append(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 ms ± 162 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.37 s ± 28.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_sdcorr_from_internal(internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ``probability_from_internal``\n",
    "\n",
    "Let $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto \\frac{1}{x^\\top 1} x$, with $1$ denoting a vector of all ones. Define $\\sigma := x^\\top 1 = \\sum_k x_k$. Then,\n",
    "$$\n",
    "J(f)(x) = \\frac{1}{\\sigma} I_m - \\frac{1}{\\sigma^2} 1 x^\\top \\,,\n",
    "$$\n",
    "where $I_m$ denotes the $m \\times m$ identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_probability_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    \n",
    "    sigma = np.sum(internal_values)\n",
    "    \n",
    "    left = np.eye(dim)\n",
    "    \n",
    "    right = np.ones((dim, dim)) * (internal_values / sigma)\n",
    "    \n",
    "    deriv = left - right.T\n",
    "    deriv /= sigma\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(probability_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17]\n"
     ]
    }
   ],
   "source": [
    "bad = []\n",
    "for dim in range(5, 50):\n",
    "    try:\n",
    "        internal = get_random_internal(dim)\n",
    "        jax_deriv = J(internal)\n",
    "        my_deriv = derivative_probability_from_internal(internal)\n",
    "        assert_array_almost_equal(jax_deriv, my_deriv, decimal=5)\n",
    "    except AssertionError:\n",
    "        bad.append(dim)\n",
    "        \n",
    "print(bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.78 ms ± 42 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 µs ± 14.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_probability_from_internal(internal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
