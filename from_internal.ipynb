{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from utilities import chol_params_to_lower_triangular_matrix\n",
    "from utilities import cov_matrix_to_sdcorr_params\n",
    "from utilities import number_of_triangular_elements_to_dimension\n",
    "from utilities import dimension_to_number_of_triangular_elements\n",
    "\n",
    "from utilities import commutation_matrix\n",
    "from utilities import elimination_matrix\n",
    "from utilities import duplication_matrix\n",
    "\n",
    "from jax import jacfwd\n",
    "from kernel_transformations_jax import covariance_from_internal as covariance_from_internal_jax\n",
    "from kernel_transformations_jax import sdcorr_from_internal as sdcorr_from_internal_jax\n",
    "from kernel_transformations_jax import probability_from_internal as probability_from_internal_jax\n",
    "\n",
    "from numpy.testing import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{\\text{vec}}\n",
    "\\left (\n",
    "\\begin{matrix}\n",
    "(0,0)  &        &        &        \\\\\n",
    "(1, 0) & (1,1)  &        &        \\\\\n",
    "(2, 0) & (2, 1) & (2, 2) &        \\\\\n",
    "(3, 0) & (3, 1) & (3, 2) & (3, 3) \\\\\n",
    "\\end{matrix}\n",
    "\\right ) =:\n",
    "\\tilde{\\text{vec}}(L) = \n",
    "\\big ( (0,0), (1,0), (1,1), (2,0), (2, 1), (2, 2), (3, 0), (3, 1), (3, 2), (3, 3) \\big )^\\top := v\n",
    "$$\n",
    "\n",
    "The following two functions allow us to move between these two representation of a (lower-triangular) matrix in a bijective fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VALUE = 500\n",
    "\n",
    "SEQUENCE_I = list(itertools.chain.from_iterable(itertools.repeat(i-1, i) for i in range(1, MAX_VALUE)))\n",
    "SEQUENCE_J = list(itertools.chain.from_iterable(range(i-1) for i in range(1, MAX_VALUE)))\n",
    "\n",
    "def _vectorized_index_to_matrix_index(index):\n",
    "    return SEQUENCE_I[index], SEQUENCE_J[index]\n",
    "\n",
    "def _matrix_index_to_vectorized_index(i, j):\n",
    "    return int(i * (i + 1) / 2) +  j\n",
    "\n",
    "for k in range(100):\n",
    "    assert _matrix_index_to_vectorized_index(*_vectorized_index_to_matrix_index(k)) == k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq(n_list, max_n=10):\n",
    "    n = len(n_list)\n",
    "    if n < max_n:\n",
    "        n_list.append(n_list[-1] + n + 1)\n",
    "        n_list = seq(n_list, max_n)\n",
    "        \n",
    "    return n_list\n",
    "\n",
    "\n",
    "TO_DROP_SEQUENCE = seq([0], max_n=500)\n",
    "SEQUENCE_SDCORR = [e for e in range(125250) if e not in TO_DROP_SEQUENCE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ``covariance_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph we want to differentiate looks as follow\n",
    "\n",
    "$$\\tilde{\\text{vec}}(L) \\to L \\to L L^\\top =: \\Sigma \\to \\tilde{\\text{vec}}(\\Sigma) \\,,$$\n",
    "where $L$ is the Cholesky factor of the covariance matrix $\\Sigma$.\n",
    "Let us define a function $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto f(x)$, which takes an internal vector $x$ (of correct dimension) and transforms it to the covariance matrix as depicted above.\n",
    "We want to find the Jacobian of $f$, i.e.\n",
    "$$\n",
    "J(f) = \\left ( \\frac{\\partial \\, f_i}{\\partial \\, x_j} \\right )_{i, j = 1, \\dots, m}\n",
    "$$\n",
    "\n",
    "We tackle this problem by finding an explicit expression for ${\\partial f_i}/{\\partial x_j}$ and then looping over $i,j = 1,\\dots, m$.\n",
    "\n",
    "Henceforth let $i, j$ be given and let $\\Sigma = (\\sigma_{i, j})$.\n",
    "Note that for each $j$ we can find a unique index tuple $(a, b) = (a(j), b(j))$ such that $\\tilde{\\text{vec}}(L)_j = L_{a, b}$ and equivalently we find $(n, m) = (n(i), m(i))$ such that $\\tilde{\\text{vec}}(\\Sigma)_i = \\sigma_{n, m}$. Also note that for these indices it always holds that $a \\geq b$ and $n \\geq m$.\n",
    "\n",
    "Now note again that with $L = \\left [ \\begin{matrix} \\ell_1 \\\\ \\vdots \\\\ \\ell_m \\end{matrix} \\right ]$, we have $\\sigma_{k, l} = \\ell_k^ \\, \\bullet \\ell_l$.\n",
    "Hence we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, f_i}{\\partial \\, x_j} = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\ell_n \\bullet \\, \\ell_m \\right ) = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\sum_{k=1}^{m} L_{n, k} L_{m, k} \\right ) = \n",
    "\\mathbb{1}(b \\leq m) \\left [ \\mathbb{1}(a = n) L_{m, b} + \\mathbb{1}(a = m) L_{n, b} \\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_covariance_from_internal(internal_values):\n",
    "    chol = chol_params_to_lower_triangular_matrix(internal_values)\n",
    "    \n",
    "    dim = len(chol)\n",
    "    \n",
    "    K = commutation_matrix(dim)\n",
    "    \n",
    "    d0_a = np.eye(dim ** 2) + K\n",
    "    d0_b = np.kron(chol, np.eye(dim))\n",
    "    d0 = d0_a @ d0_b\n",
    "    \n",
    "    L = elimination_matrix(dim)\n",
    "    D = duplication_matrix(dim, L)\n",
    "    \n",
    "    d1 = L @ d0 @ D\n",
    "    \n",
    "    return d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(covariance_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_internal(dim, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    chol = np.tril(np.random.randn(dim, dim))\n",
    "    internal = chol[np.tril_indices(len(chol))]\n",
    "    return internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(10, 50):\n",
    "    internal = get_random_internal(dim)\n",
    "\n",
    "    jax_deriv = J(internal)\n",
    "\n",
    "    my_deriv = derivative_covariance_from_internal(internal)\n",
    "\n",
    "    assert_array_almost_equal(jax_deriv, my_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.4 ms ± 187 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.7 ms ± 332 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_covariance_from_internal(internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of ``sdcorr_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph we want to differentiate looks as follow\n",
    "\n",
    "$$\\tilde{\\text{vec}}(L) \\to L \\to L L^\\top =: \\Sigma \\to \\mathcal{P} \\to \\bar{\\text{vec}}(\\mathcal{P}) \\,,$$\n",
    "where $L$ is the Cholesky factor of the covariance matrix $\\Sigma$ and $\\mathcal{P}$ denotes the *modified* correlation matrix. With modified correlation matrix we mean a correlation matrix that has the standard deviations written on the diagonal instead of ones. Moreover, the new operate $\\bar{\\text{vec}}$ maps the diagonal elements of a matrix to the first entries of the resulting vector and proceeds to fill the remaining entries using the operator $\\tilde{\\text{vec}}$. That is, if we let $M$ be some lower-triangular, $m \\times m$ matrix and define $M'$ as the $(m-1) \\times (m-1)$ lower-triangular matrix containing the lower-triangular elements of $M$ except for the diagonal elements, then\n",
    "\n",
    "$$ \\bar{\\text{vec}}(M) = \\left ( \\text{diag}(M), \\tilde{\\text{vec}}(M') \\right )$$\n",
    "\n",
    "\n",
    "Let us define a function $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto f(x)$, which takes an internal vector $x$ (of correct dimension) and transforms it to the correlation matrix as depicted above.\n",
    "We want to find the Jacobian of $f$, i.e.\n",
    "$$\n",
    "J(f) = \\left ( \\frac{\\partial \\, f_i}{\\partial \\, x_j} \\right )_{i, j = 1, \\dots, m}\n",
    "$$\n",
    "\n",
    "We tackle this problem by finding an explicit expression for ${\\partial f_i}/{\\partial x_j}$ and then looping over $i,j = 1,\\dots, m$.\n",
    "\n",
    "Henceforth let $i, j$ be given and let $\\Sigma = (\\sigma_{i, j})$ as well as $\\mathcal{P} = (\\rho_{i,j})$.\n",
    "Note that for each $j$ we can find a unique index tuple $(a, b) = (a(j), b(j))$ such that $\\tilde{\\text{vec}}(L)_j = L_{a, b}$ and equivalently we find $(n, m) = (n(i), m(i))$ such that $\\bar{\\text{vec}}(\\mathcal{P})_i = \\rho_{n, m}$. Also note that for these indices it always holds that $a \\geq b$ and $n \\geq m$.\n",
    "\n",
    "Now note again that with $L = \\left [ \\begin{matrix} \\ell_1 \\\\ \\vdots \\\\ \\ell_m \\end{matrix} \\right ]$, we have $\\sigma_{k, l} = \\ell_k^ \\, \\bullet \\ell_l$.\n",
    "\n",
    "But by definition of a correlation matrix we thus have\n",
    "\n",
    "$$\n",
    "\\rho_{n, m} = \n",
    "\\frac{\\sigma_{n, m}}{\\sqrt{\\sigma_{n, n}} \\sqrt{\\sigma_{m, m}}} =\n",
    "\\frac{\\ell_n \\, \\bullet \\ell_m}{||\\ell_n||_2 \\, ||\\ell_m||_2} \\,.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, f_i}{\\partial \\, x_j} = \n",
    "\\frac{\\partial}{\\partial L_{a(j), b(j)}} \\rho_{n(i), m(i)} =\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\frac{\\ell_n \\, \\bullet \\ell_m}{||\\ell_n||_2 \\, ||\\ell_m||_2}\n",
    "\\right ) =: \n",
    "(\\star)$$\n",
    "\n",
    "To solve for $(\\star)$ let us consider first\n",
    "\n",
    "$$\n",
    "(\\star \\star) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\ell_n \\, \\bullet \\ell_m \\right ) = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\sum_{k=1}^{m} L_{n,k} L_{m, k} \\right ) = \n",
    "\\mathbb{1}(b \\leq m, a = n) L_{m, b} + \\mathbb{1}(b \\leq m, a = m) L_{n, b}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "(\\bullet) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} ||\\ell_n||_2 = \\frac{\\partial}{\\partial L_{a, b}} \\sqrt{\\sum_{k=1}^n L_{n, k}^2} = \\mathbb{1}(b \\leq n, a = n) L_{n, b} ||\\ell_n||_2^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "(\\bullet \\, \\bullet) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_n||_2 \\, ||\\ell_m||_2 \\right ) &= \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_n||_2 \\right ) ||\\ell_m||_2 + \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_m||_2 \\right ) ||\\ell_n||_2 \\\\\n",
    "&= \\mathbb{1}(b \\leq n, a = n) L_{n, b} \\frac{||\\ell_m||_2}{||\\ell_n||_2} + \n",
    "\\mathbb{1}(b \\leq m, a = m) L_{m, b}  \\frac{||\\ell_n||_2}{||\\ell_m||_2}\n",
    "\\end{align}\n",
    "\n",
    "Then, with $\\alpha_{n, m} := ||\\ell_n||_2 \\, ||\\ell_m||_2$, we get by applying the quotient rule\n",
    "\n",
    "$$\n",
    "(\\star) = \\frac{1}{\\alpha_{n, m}^2} \\left ( \\alpha_{n, m} \\times (\\star \\star) - \\ell_n \\, \\bullet \\ell_m \\times (\\bullet \\, \\bullet) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sdcorr_index(index, dim):\n",
    "    if index < dim:\n",
    "        i, j = index, index\n",
    "    else:\n",
    "        idx = SEQUENCE_SDCORR[index - dim]\n",
    "        i, j = _vectorized_index_to_matrix_index(idx)\n",
    "    return i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sdcorr_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    \n",
    "    chol = chol_params_to_lower_triangular_matrix(internal_values)\n",
    "    n_chol = len(chol)\n",
    "    \n",
    "    deriv = np.zeros((dim, dim))\n",
    "    for i in range(dim):\n",
    "        \n",
    "        n, m = _sdcorr_index(i, n_chol)\n",
    "        \n",
    "        for j in range(dim):\n",
    "            \n",
    "            a, b = _vectorized_index_to_matrix_index(j)\n",
    "            \n",
    "            if i < n_chol:\n",
    "                deriv[i, j] = _derivative_sdcorr_from_internal_inner_sd(n, m, a, b, chol)\n",
    "            else:\n",
    "                deriv[i, j] = _derivative_sdcorr_from_internal_inner_corr(n, m, a, b, chol)\n",
    "\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derivative_sdcorr_from_internal_inner_sd(n, m, a, b, chol): \n",
    "    if b <= n and a == n:\n",
    "        ln_norm = np.sqrt(np.sum(chol[n] ** 2))\n",
    "        deriv = chol[n, b] / ln_norm\n",
    "    else:\n",
    "        deriv = 0\n",
    "    \n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derivative_sdcorr_from_internal_inner_corr(n, m, a, b, chol):\n",
    "    ln_norm = np.sqrt(np.sum(chol[n] ** 2))\n",
    "    lm_norm = np.sqrt(np.sum(chol[m] ** 2))\n",
    "\n",
    "    alpha = ln_norm * lm_norm\n",
    "\n",
    "    # \\ell_n \\bullet \\ell_m\n",
    "    dotprod = np.dot(chol[n], chol[m])\n",
    "\n",
    "\n",
    "    # (\\star \\star)\n",
    "    left = 0\n",
    "    if b <= m:\n",
    "        if a == n:\n",
    "            left += chol[m, b]\n",
    "        if a == m:\n",
    "            left += chol[n, b]\n",
    "\n",
    "    # (\\bullet \\bullet)\n",
    "    right = 0\n",
    "    if b <= n and a == n:\n",
    "        right += chol[n, b] * lm_norm / ln_norm\n",
    "    if b <= m and a == m:\n",
    "        right += chol[m, b] * ln_norm / lm_norm\n",
    "\n",
    "    deriv = (alpha * left - dotprod * right) / (alpha ** 2)\n",
    "\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(sdcorr_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(10, 50):\n",
    "    internal = get_random_internal(dim)\n",
    "    jax_deriv = J(internal)\n",
    "    my_deriv = derivative_sdcorr_from_internal(internal)\n",
    "    assert_array_almost_equal(jax_deriv, my_deriv)\n",
    "    bad.append(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 ms ± 162 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.37 s ± 28.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_sdcorr_from_internal(internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ``probability_from_internal``\n",
    "\n",
    "Let $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto \\frac{1}{x^\\top 1} x$, with $1$ denoting a vector of all ones. Define $\\sigma := x^\\top 1 = \\sum_k x_k$. Then,\n",
    "$$\n",
    "J(f)(x) = \\frac{1}{\\sigma} I_m - \\frac{1}{\\sigma^2} 1 x^\\top \\,,\n",
    "$$\n",
    "where $I_m$ denotes the $m \\times m$ identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_probability_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    \n",
    "    sigma = np.sum(internal_values)\n",
    "    \n",
    "    left = np.eye(dim)\n",
    "    \n",
    "    right = np.ones((dim, dim)) * (internal_values / sigma)\n",
    "    \n",
    "    deriv = left - right.T\n",
    "    deriv /= sigma\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(probability_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17]\n"
     ]
    }
   ],
   "source": [
    "bad = []\n",
    "for dim in range(5, 50):\n",
    "    try:\n",
    "        internal = get_random_internal(dim)\n",
    "        jax_deriv = J(internal)\n",
    "        my_deriv = derivative_probability_from_internal(internal)\n",
    "        assert_array_almost_equal(jax_deriv, my_deriv, decimal=5)\n",
    "    except AssertionError:\n",
    "        bad.append(dim)\n",
    "        \n",
    "print(bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.78 ms ± 42 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 µs ± 14.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_probability_from_internal(internal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
