{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "from utilities import chol_params_to_lower_triangular_matrix\n",
    "from utilities import cov_matrix_to_sdcorr_params\n",
    "from utilities import number_of_triangular_elements_to_dimension\n",
    "\n",
    "from jax import jacfwd\n",
    "from kernel_transformations_jax import covariance_from_internal as covariance_from_internal_jax\n",
    "from kernel_transformations_jax import sdcorr_from_internal as sdcorr_from_internal_jax\n",
    "from kernel_transformations_jax import probability_from_internal as probability_from_internal_jax\n",
    "\n",
    "from numpy.testing import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{\\text{vec}}\n",
    "\\left (\n",
    "\\begin{matrix}\n",
    "(0,0)  &        &        &        \\\\\n",
    "(1, 0) & (1,1)  &        &        \\\\\n",
    "(2, 0) & (2, 1) & (2, 2) &        \\\\\n",
    "(3, 0) & (3, 1) & (3, 2) & (3, 3) \\\\\n",
    "\\end{matrix}\n",
    "\\right ) =:\n",
    "\\tilde{\\text{vec}}(L) = \n",
    "\\big ( (0,0), (1,0), (1,1), (2,0), (2, 1), (2, 2), (3, 0), (3, 1), (3, 2), (3, 3) \\big )^\\top := v\n",
    "$$\n",
    "\n",
    "The following two functions allow us to move between these two representation of a (lower-triangular) matrix in a bijective fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VALUE = 500\n",
    "\n",
    "SEQUENCE_I = list(itertools.chain.from_iterable(itertools.repeat(i-1, i) for i in range(1, MAX_VALUE)))\n",
    "SEQUENCE_J = list(itertools.chain.from_iterable(range(i-1) for i in range(1, MAX_VALUE)))\n",
    "\n",
    "SEQUENCE_I = np.array(SEQUENCE_I)\n",
    "SEQUENCE_J = np.array(SEQUENCE_J)\n",
    "\n",
    "@njit\n",
    "def _vectorized_index_to_matrix_index(index):\n",
    "    return np.array([SEQUENCE_I[index], SEQUENCE_J[index]])\n",
    "\n",
    "@njit\n",
    "def _matrix_index_to_vectorized_index(i, j):\n",
    "    return int(i * (i + 1) / 2) +  j\n",
    "\n",
    "for k in range(100):\n",
    "    assert _matrix_index_to_vectorized_index(*_vectorized_index_to_matrix_index(k)) == k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ``covariance_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph we want to differentiate looks as follow\n",
    "\n",
    "$$\\tilde{\\text{vec}}(L) \\to L \\to L L^\\top =: \\Sigma \\to \\tilde{\\text{vec}}(\\Sigma) \\,,$$\n",
    "where $L$ is the Cholesky factor of the covariance matrix $\\Sigma$.\n",
    "Let us define a function $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto f(x)$, which takes an internal vector $x$ (of correct dimension) and transforms it to the covariance matrix as depicted above.\n",
    "We want to find the Jacobian of $f$, i.e.\n",
    "$$\n",
    "J(f) = \\left ( \\frac{\\partial \\, f_i}{\\partial \\, x_j} \\right )_{i, j = 1, \\dots, m}\n",
    "$$\n",
    "\n",
    "We tackle this problem by finding an explicit expression for ${\\partial f_i}/{\\partial x_j}$ and then looping over $i,j = 1,\\dots, m$.\n",
    "\n",
    "Henceforth let $i, j$ be given and let $\\Sigma = (\\sigma_{i, j})$.\n",
    "Note that for each $j$ we can find a unique index tuple $(a, b) = (a(j), b(j))$ such that $\\tilde{\\text{vec}}(L)_j = L_{a, b}$ and equivalently we find $(n, m) = (n(i), m(i))$ such that $\\tilde{\\text{vec}}(\\Sigma)_i = \\sigma_{n, m}$. Also note that for these indices it always holds that $a \\geq b$ and $n \\geq m$.\n",
    "\n",
    "Now note again that with $L = \\left [ \\begin{matrix} \\ell_1 \\\\ \\vdots \\\\ \\ell_m \\end{matrix} \\right ]$, we have $\\sigma_{k, l} = \\ell_k^ \\, \\bullet \\ell_l$.\n",
    "Hence we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, f_i}{\\partial \\, x_j} = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\ell_n \\bullet \\, \\ell_m \\right ) = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\sum_{k=1}^{m} L_{n, k} L_{m, k} \\right ) = \n",
    "\\mathbb{1}(b \\leq m) \\left [ \\mathbb{1}(a = n) L_{m, b} + \\mathbb{1}(a = m) L_{n, b} \\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_covariance_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    \n",
    "    chol = chol_params_to_lower_triangular_matrix(internal_values)\n",
    "    \n",
    "    deriv = np.zeros((dim, dim))\n",
    "    \n",
    "    for i in range(dim):\n",
    "        \n",
    "        outer_index = _vectorized_index_to_matrix_index(i)\n",
    "        n = outer_index[0]\n",
    "        m = outer_index[1]\n",
    "        \n",
    "        for j in range(dim):\n",
    "            \n",
    "            inner_index = _vectorized_index_to_matrix_index(j)\n",
    "            a = inner_index[0]\n",
    "            b = inner_index[1]\n",
    "            \n",
    "            deriv[i, j] = _derivative_covariance_from_internal_inner(n, m, a, b, chol)\n",
    "                \n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derivative_covariance_from_internal_inner(n, m, a, b, chol):\n",
    "    deriv = 0\n",
    "    \n",
    "    if b <= m:\n",
    "        if a == n:\n",
    "            deriv += chol[m, b]\n",
    "        if a == m:\n",
    "            deriv += chol[n, b]\n",
    "        \n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(covariance_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_internal(dim, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    chol = np.tril(np.random.randn(dim, dim))\n",
    "    internal = chol[np.tril_indices(len(chol))]\n",
    "    return internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in range(10, 50):\n",
    "    internal = get_random_internal(dim)\n",
    "\n",
    "    jax_deriv = J(internal)\n",
    "\n",
    "    my_deriv = derivative_covariance_from_internal(internal)\n",
    "\n",
    "    assert_array_almost_equal(jax_deriv, my_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5 ms ± 484 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.3 ms ± 2.24 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_covariance_from_internal(internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of ``sdcorr_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph we want to differentiate looks as follow\n",
    "\n",
    "$$\\tilde{\\text{vec}}(L) \\to L \\to L L^\\top =: \\Sigma \\to \\mathcal{P} \\to \\tilde{\\text{vec}}(\\mathcal{P}) \\,,$$\n",
    "where $L$ is the Cholesky factor of the covariance matrix $\\Sigma$ and $\\mathcal{P}$ denotes the correlation matrix.\n",
    "Let us define a function $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto f(x)$, which takes an internal vector $x$ (of correct dimension) and transforms it to the correlation matrix as depicted above.\n",
    "We want to find the Jacobian of $f$, i.e.\n",
    "$$\n",
    "J(f) = \\left ( \\frac{\\partial \\, f_i}{\\partial \\, x_j} \\right )_{i, j = 1, \\dots, m}\n",
    "$$\n",
    "\n",
    "We tackle this problem by finding an explicit expression for ${\\partial f_i}/{\\partial x_j}$ and then looping over $i,j = 1,\\dots, m$.\n",
    "\n",
    "Henceforth let $i, j$ be given and let $\\Sigma = (\\sigma_{i, j})$ as well as $\\mathcal{P} = (\\rho_{i,j})$.\n",
    "Note that for each $j$ we can find a unique index tuple $(a, b) = (a(j), b(j))$ such that $\\tilde{\\text{vec}}(L)_j = L_{a, b}$ and equivalently we find $(n, m) = (n(i), m(i))$ such that $\\tilde{\\text{vec}}(\\mathcal{P})_i = \\rho_{n, m}$. Also note that for these indices it always holds that $a \\geq b$ and $n \\geq m$.\n",
    "\n",
    "Now note again that with $L = \\left [ \\begin{matrix} \\ell_1 \\\\ \\vdots \\\\ \\ell_m \\end{matrix} \\right ]$, we have $\\sigma_{k, l} = \\ell_k^ \\, \\bullet \\ell_l$.\n",
    "\n",
    "But by definition of a correlation matrix we thus have\n",
    "\n",
    "$$\n",
    "\\rho_{n, m} = \n",
    "\\frac{\\sigma_{n, m}}{\\sqrt{\\sigma_{n, n}} \\sqrt{\\sigma_{m, m}}} =\n",
    "\\frac{\\ell_n \\, \\bullet \\ell_m}{||\\ell_n||_2 \\, ||\\ell_m||_2} \\,.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, f_i}{\\partial \\, x_j} = \n",
    "\\frac{\\partial}{\\partial L_{a(j), b(j)}} \\rho_{n(i), m(i)} =\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\frac{\\ell_n \\, \\bullet \\ell_m}{||\\ell_n||_2 \\, ||\\ell_m||_2}\n",
    "\\right ) =: \n",
    "(\\star)$$\n",
    "\n",
    "To solve for $(\\star)$ let us consider first\n",
    "\n",
    "$$\n",
    "(\\star \\star) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\ell_n \\, \\bullet \\ell_m \\right ) = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\sum_{k=1}^{m} L_{n,k} L_{m, k} \\right ) = \n",
    "\\mathbb{1}(b \\leq m, a = n) L_{m, b} + \\mathbb{1}(b \\leq m, a = m) L_{n, b}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "(\\bullet) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} ||\\ell_n||_2 = \\frac{\\partial}{\\partial L_{a, b}} \\sqrt{\\sum_{k=1}^n L_{n, k}^2} = \\mathbb{1}(b \\leq n, a = n) L_{a, b}\n",
    "$$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "(\\bullet \\, \\bullet) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_n||_2 \\, ||\\ell_m||_2 \\right ) &= \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_n||_2 \\right ) ||\\ell_m||_2 + \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_m||_2 \\right ) ||\\ell_n||_2 \\\\\n",
    "&= L_{a, b} \\left ( \\mathbb{1}(b \\leq n, a = n) ||\\ell_m||_2 + \n",
    "\\mathbb{1}(b \\leq m, a = m) ||\\ell_n||_2 \\right )\n",
    "\\end{align}\n",
    "\n",
    "Then, with $\\alpha_{n, m} := ||\\ell_n||_2 \\, ||\\ell_m||_2$, we get by applying the quotient rule\n",
    "\n",
    "$$\n",
    "(\\star) = \\frac{1}{\\alpha_{n, m}^2} \\left ( \\alpha_{n, m} \\times (\\star \\star) - \\ell_n \\, \\bullet \\ell_m \\times (\\bullet \\, \\bullet) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sdcorr_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    chol = chol_params_to_lower_triangular_matrix(internal_values)\n",
    "    deriv = _derivative_sdcorr_from_internal_outer(dim, chol)\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derivative_sdcorr_from_internal_outer(dim, chol):\n",
    "    deriv = np.zeros((dim, dim))\n",
    "    \n",
    "    for i in range(dim):\n",
    "        \n",
    "        outer_index = _vectorized_index_to_matrix_index(i)\n",
    "        n = outer_index[0]\n",
    "        m = outer_index[1]\n",
    "        \n",
    "        for j in range(dim):\n",
    "            \n",
    "            inner_index = _vectorized_index_to_matrix_index(j)\n",
    "            a = inner_index[0]\n",
    "            b = inner_index[1]\n",
    "            \n",
    "            deriv[i, j] = _derivative_sdcorr_from_internal_inner(n, m, a, b, chol)\n",
    "                \n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derivative_sdcorr_from_internal_inner(n, m, a, b, chol):\n",
    "    ln_norm = np.sqrt(np.sum(chol[n] ** 2))\n",
    "    lm_norm = np.sqrt(np.sum(chol[m] ** 2))\n",
    "    \n",
    "    alpha = ln_norm * lm_norm\n",
    "    \n",
    "    # \\ell_n \\bullet \\ell_m\n",
    "    dotprod = np.dot(chol[n], chol[m])\n",
    "    \n",
    "    \n",
    "    # (\\star \\star)\n",
    "    left = 0\n",
    "    if b <= m:\n",
    "        if a == n:\n",
    "            left += chol[m, b]\n",
    "        if a == m:\n",
    "            left += chol[n, b]\n",
    "            \n",
    "    # (\\bullet \\bullet)\n",
    "    right = 0\n",
    "    if b <= n and a == n:\n",
    "        right += lm_norm\n",
    "    if b <= m and a == m:\n",
    "        right += ln_norm\n",
    "    right *= chol[a, b]\n",
    "\n",
    "    deriv = (alpha * left - dotprod * right) / (alpha ** 2)\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(sdcorr_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n\nMismatch: 25.2%\nMax absolute difference: 1.17721257\nMax relative difference: nan\n x: array([[ 1.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n         0.      ],\n       [ 0.      ,  0.098566,  0.99513 , ...,  0.      ,  0.      ,...\n y: array([[0.866247, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n       [0.994493, 0.031119, 0.      , ..., 0.      , 0.      , 0.      ],\n       [0.      , 0.062239, 0.628365, ..., 0.      , 0.      , 0.      ],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-d802d4d3b0e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmy_deriv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mderivative_sdcorr_from_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0massert_array_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax_deriv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_deriv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\nMismatch: 25.2%\nMax absolute difference: 1.17721257\nMax relative difference: nan\n x: array([[ 1.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n         0.      ],\n       [ 0.      ,  0.098566,  0.99513 , ...,  0.      ,  0.      ,...\n y: array([[0.866247, 0.      , 0.      , ..., 0.      , 0.      , 0.      ],\n       [0.994493, 0.031119, 0.      , ..., 0.      , 0.      , 0.      ],\n       [0.      , 0.062239, 0.628365, ..., 0.      , 0.      , 0.      ],..."
     ]
    }
   ],
   "source": [
    "for dim in range(10, 11):\n",
    "    internal = get_random_internal(dim)\n",
    "\n",
    "    jax_deriv = J(internal)\n",
    "\n",
    "    my_deriv = derivative_sdcorr_from_internal(internal)\n",
    "\n",
    "    assert_array_almost_equal(jax_deriv, my_deriv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = np.array([1, 2, 4, 2, 0.5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "             [ 0.  ,  0.77,  0.64,  0.  ,  0.  ,  0.  ],\n",
       "             [ 0.  ,  0.  ,  0.  ,  0.98, -0.16, -0.11],\n",
       "             [ 0.  ,  0.14, -0.17,  0.  ,  0.  ,  0.  ],\n",
       "             [ 0.  ,  0.  ,  0.  ,  0.04,  0.16,  0.11],\n",
       "             [ 0.  ,  0.16, -0.2 ,  0.13,  0.77,  0.07]], dtype=float32)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J(internal).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.87,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [-0.33, -0.25, -0.49,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  , -1.01, -0.84,  0.  ,  0.  ,  0.  ],\n",
       "       [-0.43,  0.  ,  0.  ,  0.07,  0.15,  0.1 ],\n",
       "       [ 0.  , -0.17, -0.47,  0.15,  0.76,  0.07],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.07, -0.01, -0.01]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_sdcorr_from_internal(internal).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ``probability_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto \\frac{1}{x^\\top 1} x$, with $1$ denoting a vector of all ones. Define $\\sigma := x^\\top 1 = \\sum_k x_k$. Then,\n",
    "$$\n",
    "J(f)(x) = \\frac{1}{\\sigma} I_m - \\frac{1}{\\sigma^2} 1 x^\\top \\,,\n",
    "$$\n",
    "where $I_m$ denotes the $m \\times m$ identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_probability_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    \n",
    "    sigma = np.sum(internal_values)\n",
    "    \n",
    "    left = np.eye(dim) / sigma\n",
    "    \n",
    "    right = np.ones((dim, dim)) * (internal_values / (sigma ** 2))\n",
    "    \n",
    "    deriv = left - right.T\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(probability_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 21, 34, 39]\n"
     ]
    }
   ],
   "source": [
    "bad = []\n",
    "for dim in range(10, 50):\n",
    "    try:\n",
    "        internal = get_random_internal(dim)\n",
    "        jax_deriv = J(internal)\n",
    "        my_deriv = derivative_probability_from_internal(internal)\n",
    "        assert_array_almost_equal(jax_deriv, my_deriv)\n",
    "    except AssertionError:\n",
    "        bad.append(dim)\n",
    "        \n",
    "print(bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9 ms ± 127 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 ms ± 37.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_probability_from_internal(internal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
