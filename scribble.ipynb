{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "\n",
    "from utilities import chol_params_to_lower_triangular_matrix\n",
    "from utilities import cov_matrix_to_sdcorr_params\n",
    "from utilities import number_of_triangular_elements_to_dimension\n",
    "\n",
    "from jax import jacfwd\n",
    "from kernel_transformations_jax import covariance_from_internal as covariance_from_internal_jax\n",
    "from kernel_transformations_jax import sdcorr_from_internal as sdcorr_from_internal_jax\n",
    "from kernel_transformations_jax import probability_from_internal as probability_from_internal_jax\n",
    "\n",
    "from numpy.testing import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{\\text{vec}}\n",
    "\\left (\n",
    "\\begin{matrix}\n",
    "(0,0)  &        &        &        \\\\\n",
    "(1, 0) & (1,1)  &        &        \\\\\n",
    "(2, 0) & (2, 1) & (2, 2) &        \\\\\n",
    "(3, 0) & (3, 1) & (3, 2) & (3, 3) \\\\\n",
    "\\end{matrix}\n",
    "\\right ) =:\n",
    "\\tilde{\\text{vec}}(L) = \n",
    "\\big ( (0,0), (1,0), (1,1), (2,0), (2, 1), (2, 2), (3, 0), (3, 1), (3, 2), (3, 3) \\big )^\\top := v\n",
    "$$\n",
    "\n",
    "The following two functions allow us to move between these two representation of a (lower-triangular) matrix in a bijective fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VALUE = 500\n",
    "\n",
    "SEQUENCE_I = list(itertools.chain.from_iterable(itertools.repeat(i-1, i) for i in range(1, MAX_VALUE)))\n",
    "SEQUENCE_J = list(itertools.chain.from_iterable(range(i-1) for i in range(1, MAX_VALUE)))\n",
    "\n",
    "SEQUENCE_I = np.array(SEQUENCE_I)\n",
    "SEQUENCE_J = np.array(SEQUENCE_J)\n",
    "\n",
    "@njit\n",
    "def _vectorized_index_to_matrix_index(index):\n",
    "    return np.array([SEQUENCE_I[index], SEQUENCE_J[index]])\n",
    "\n",
    "@njit\n",
    "def _matrix_index_to_vectorized_index(i, j):\n",
    "    return int(i * (i + 1) / 2) +  j\n",
    "\n",
    "for k in range(100):\n",
    "    assert _matrix_index_to_vectorized_index(*_vectorized_index_to_matrix_index(k)) == k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ``covariance_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph we want to differentiate looks as follow\n",
    "\n",
    "$$\\tilde{\\text{vec}}(L) \\to L \\to L L^\\top =: \\Sigma \\to \\tilde{\\text{vec}}(\\Sigma) \\,,$$\n",
    "where $L$ is the Cholesky factor of the covariance matrix $\\Sigma$.\n",
    "Let us define a function $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto f(x)$, which takes an internal vector $x$ (of correct dimension) and transforms it to the covariance matrix as depicted above.\n",
    "We want to find the Jacobian of $f$, i.e.\n",
    "$$\n",
    "J(f) = \\left ( \\frac{\\partial \\, f_i}{\\partial \\, x_j} \\right )_{i, j = 1, \\dots, m}\n",
    "$$\n",
    "\n",
    "We tackle this problem by finding an explicit expression for ${\\partial f_i}/{\\partial x_j}$ and then looping over $i,j = 1,\\dots, m$.\n",
    "\n",
    "Henceforth let $i, j$ be given and let $\\Sigma = (\\sigma_{i, j})$.\n",
    "Note that for each $j$ we can find a unique index tuple $(a, b) = (a(j), b(j))$ such that $\\tilde{\\text{vec}}(L)_j = L_{a, b}$ and equivalently we find $(n, m) = (n(i), m(i))$ such that $\\tilde{\\text{vec}}(\\Sigma)_i = \\sigma_{n, m}$. Also note that for these indices it always holds that $a \\geq b$ and $n \\geq m$.\n",
    "\n",
    "Now note again that with $L = \\left [ \\begin{matrix} \\ell_1 \\\\ \\vdots \\\\ \\ell_m \\end{matrix} \\right ]$, we have $\\sigma_{k, l} = \\ell_k^ \\, \\bullet \\ell_l$.\n",
    "Hence we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, f_i}{\\partial \\, x_j} = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\ell_n \\bullet \\, \\ell_m \\right ) = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\sum_{k=1}^{m} L_{n, k} L_{m, k} \\right ) = \n",
    "\\mathbb{1}(b \\leq m) \\left [ \\mathbb{1}(a = n) L_{m, b} + \\mathbb{1}(a = m) L_{n, b} \\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_covariance_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    \n",
    "    chol = chol_params_to_lower_triangular_matrix(internal_values)\n",
    "    \n",
    "    deriv = np.zeros((dim, dim))\n",
    "    \n",
    "    for i in range(dim):\n",
    "        \n",
    "        outer_index = _vectorized_index_to_matrix_index(i)\n",
    "        n = outer_index[0]\n",
    "        m = outer_index[1]\n",
    "        \n",
    "        for j in range(dim):\n",
    "            \n",
    "            inner_index = _vectorized_index_to_matrix_index(j)\n",
    "            a = inner_index[0]\n",
    "            b = inner_index[1]\n",
    "            \n",
    "            deriv[i, j] = _derivative_covariance_from_internal_inner(n, m, a, b, chol)\n",
    "                \n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derivative_covariance_from_internal_inner(n, m, a, b, chol):\n",
    "    deriv = 0\n",
    "    \n",
    "    if b <= m:\n",
    "        if a == n:\n",
    "            deriv += chol[m, b]\n",
    "        if a == m:\n",
    "            deriv += chol[n, b]\n",
    "        \n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(covariance_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_internal(dim, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    chol = np.tril(np.random.randn(dim, dim))\n",
    "    internal = chol[np.tril_indices(len(chol))]\n",
    "    return internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tm/anaconda3/envs/derivatives/lib/python3.7/site-packages/jax/lib/xla_bridge.py:125: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "for dim in range(10, 50):\n",
    "    internal = get_random_internal(dim)\n",
    "\n",
    "    jax_deriv = J(internal)\n",
    "\n",
    "    my_deriv = derivative_covariance_from_internal(internal)\n",
    "\n",
    "    assert_array_almost_equal(jax_deriv, my_deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5 ms ± 65.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.8 ms ± 630 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_covariance_from_internal(internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative of ``sdcorr_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph we want to differentiate looks as follow\n",
    "\n",
    "$$\\tilde{\\text{vec}}(L) \\to L \\to L L^\\top =: \\Sigma \\to \\mathcal{P} \\to \\tilde{\\text{vec}}(\\mathcal{P}) \\,,$$\n",
    "where $L$ is the Cholesky factor of the covariance matrix $\\Sigma$ and $\\mathcal{P}$ denotes the correlation matrix.\n",
    "Let us define a function $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto f(x)$, which takes an internal vector $x$ (of correct dimension) and transforms it to the correlation matrix as depicted above.\n",
    "We want to find the Jacobian of $f$, i.e.\n",
    "$$\n",
    "J(f) = \\left ( \\frac{\\partial \\, f_i}{\\partial \\, x_j} \\right )_{i, j = 1, \\dots, m}\n",
    "$$\n",
    "\n",
    "We tackle this problem by finding an explicit expression for ${\\partial f_i}/{\\partial x_j}$ and then looping over $i,j = 1,\\dots, m$.\n",
    "\n",
    "Henceforth let $i, j$ be given and let $\\Sigma = (\\sigma_{i, j})$ as well as $\\mathcal{P} = (\\rho_{i,j})$.\n",
    "Note that for each $j$ we can find a unique index tuple $(a, b) = (a(j), b(j))$ such that $\\tilde{\\text{vec}}(L)_j = L_{a, b}$ and equivalently we find $(n, m) = (n(i), m(i))$ such that $\\tilde{\\text{vec}}(\\mathcal{P})_i = \\rho_{n, m}$. Also note that for these indices it always holds that $a \\geq b$ and $n \\geq m$.\n",
    "\n",
    "Now note again that with $L = \\left [ \\begin{matrix} \\ell_1 \\\\ \\vdots \\\\ \\ell_m \\end{matrix} \\right ]$, we have $\\sigma_{k, l} = \\ell_k^ \\, \\bullet \\ell_l$.\n",
    "\n",
    "But by definition of a correlation matrix we thus have\n",
    "\n",
    "$$\n",
    "\\rho_{n, m} = \n",
    "\\frac{\\sigma_{n, m}}{\\sqrt{\\sigma_{n, n}} \\sqrt{\\sigma_{m, m}}} =\n",
    "\\frac{\\ell_n \\, \\bullet \\ell_m}{||\\ell_n||_2 \\, ||\\ell_m||_2} \\,.\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\, f_i}{\\partial \\, x_j} = \n",
    "\\frac{\\partial}{\\partial L_{a(j), b(j)}} \\rho_{n(i), m(i)} =\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\frac{\\ell_n \\, \\bullet \\ell_m}{||\\ell_n||_2 \\, ||\\ell_m||_2}\n",
    "\\right ) =: \n",
    "(\\star)$$\n",
    "\n",
    "To solve for $(\\star)$ let us consider first\n",
    "\n",
    "$$\n",
    "(\\star \\star) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\ell_n \\, \\bullet \\ell_m \\right ) = \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( \\sum_{k=1}^{m} L_{n,k} L_{m, k} \\right ) = \n",
    "\\mathbb{1}(b \\leq m, a = n) L_{m, b} + \\mathbb{1}(b \\leq m, a = m) L_{n, b}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "(\\bullet) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} ||\\ell_n||_2 = \\frac{\\partial}{\\partial L_{a, b}} \\sqrt{\\sum_{k=1}^n L_{n, k}^2} = \\mathbb{1}(b \\leq n, a = n) L_{n, b} ||\\ell_n||_2^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "(\\bullet \\, \\bullet) :=\n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_n||_2 \\, ||\\ell_m||_2 \\right ) &= \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_n||_2 \\right ) ||\\ell_m||_2 + \n",
    "\\frac{\\partial}{\\partial L_{a, b}} \\left ( ||\\ell_m||_2 \\right ) ||\\ell_n||_2 \\\\\n",
    "&= \\mathbb{1}(b \\leq n, a = n) L_{n, b} \\frac{||\\ell_m||_2}{||\\ell_n||_2} + \n",
    "\\mathbb{1}(b \\leq m, a = m) L_{m, b}  \\frac{||\\ell_n||_2}{||\\ell_m||_2}\n",
    "\\end{align}\n",
    "\n",
    "Then, with $\\alpha_{n, m} := ||\\ell_n||_2 \\, ||\\ell_m||_2$, we get by applying the quotient rule\n",
    "\n",
    "$$\n",
    "(\\star) = \\frac{1}{\\alpha_{n, m}^2} \\left ( \\alpha_{n, m} \\times (\\star \\star) - \\ell_n \\, \\bullet \\ell_m \\times (\\bullet \\, \\bullet) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sdcorr_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    \n",
    "    chol = chol_params_to_lower_triangular_matrix(internal_values)\n",
    "    \n",
    "    deriv = np.zeros((dim, dim))\n",
    "    \n",
    "    for i in range(dim):\n",
    "        \n",
    "        outer_index = _vectorized_index_to_matrix_index(i)\n",
    "        n = outer_index[0]\n",
    "        m = outer_index[1]\n",
    "        \n",
    "        for j in range(dim):\n",
    "            \n",
    "            inner_index = _vectorized_index_to_matrix_index(j)\n",
    "            a = inner_index[0]\n",
    "            b = inner_index[1]\n",
    "            \n",
    "            deriv[i, j] = _derivative_sdcorr_from_internal_inner(n, m, a, b, chol)\n",
    "\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _derivative_sdcorr_from_internal_inner(n, m, a, b, chol):\n",
    "    ln_norm = np.sqrt(np.sum(chol[n] ** 2))\n",
    "    lm_norm = np.sqrt(np.sum(chol[m] ** 2))\n",
    "    \n",
    "    alpha = ln_norm * lm_norm\n",
    "    \n",
    "    # \\ell_n \\bullet \\ell_m\n",
    "    dotprod = np.dot(chol[n], chol[m])\n",
    "    \n",
    "    \n",
    "    # (\\star \\star)\n",
    "    left = 0\n",
    "    if b <= m:\n",
    "        if a == n:\n",
    "            left += chol[m, b]\n",
    "        if a == m:\n",
    "            left += chol[n, b]\n",
    "            \n",
    "    # (\\bullet \\bullet)\n",
    "    right = 0\n",
    "    if b <= n and a == n:\n",
    "        right += chol[n, b] * lm_norm / ln_norm\n",
    "    if b <= m and a == m:\n",
    "        right += chol[m, b] * ln_norm / lm_norm\n",
    "\n",
    "    deriv = (alpha * left - dotprod * right) / (alpha ** 2)\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(sdcorr_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "bad = []\n",
    "for dim in range(10, 25):\n",
    "    try:\n",
    "        internal = get_random_internal(dim)\n",
    "        jax_deriv = J(internal)\n",
    "        my_deriv = derivative_sdcorr_from_internal(internal)\n",
    "        assert_array_almost_equal(jax_deriv, my_deriv)\n",
    "    except AssertionError:\n",
    "        bad.append(dim)\n",
    "        \n",
    "print(bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.6 ms ± 1.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.75 s ± 116 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_sdcorr_from_internal(internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative of ``probability_from_internal``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f: \\mathbb{R}^m \\to \\mathbb{R}^m, x \\mapsto \\frac{1}{x^\\top 1} x$, with $1$ denoting a vector of all ones. Define $\\sigma := x^\\top 1 = \\sum_k x_k$. Then,\n",
    "$$\n",
    "J(f)(x) = \\frac{1}{\\sigma} I_m - \\frac{1}{\\sigma^2} 1 x^\\top \\,,\n",
    "$$\n",
    "where $I_m$ denotes the $m \\times m$ identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_probability_from_internal(internal_values):\n",
    "    dim = len(internal_values)\n",
    "    \n",
    "    sigma = np.sum(internal_values)\n",
    "    \n",
    "    left = np.eye(dim) / sigma\n",
    "    \n",
    "    right = np.ones((dim, dim)) * (internal_values / (sigma ** 2))\n",
    "    \n",
    "    deriv = left - right.T\n",
    "    return deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = jacfwd(probability_from_internal_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 21, 34, 39]\n"
     ]
    }
   ],
   "source": [
    "bad = []\n",
    "for dim in range(10, 50):\n",
    "    try:\n",
    "        internal = get_random_internal(dim)\n",
    "        jax_deriv = J(internal)\n",
    "        my_deriv = derivative_probability_from_internal(internal)\n",
    "        assert_array_almost_equal(jax_deriv, my_deriv)\n",
    "    except AssertionError:\n",
    "        bad.append(dim)\n",
    "        \n",
    "print(bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal = get_random_internal(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.78 ms ± 42 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit J(internal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 µs ± 14.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit derivative_probability_from_internal(internal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
